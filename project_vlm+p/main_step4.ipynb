{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5f1b55-d789-4b06-bfd0-394328dc0c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 01:02:45,769 - modelscope - INFO - PyTorch version 2.0.1 Found.\n",
      "2023-09-11 01:02:45,773 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2023-09-11 01:02:45,832 - modelscope - INFO - Loading done! Current index file version is 1.9.0, with md5 324dfd599f06dff38089c5f9fda58af5 and a total number of 921 components indexed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@Description :   The 4th step of main.py, where LLM & VLM are implemented\n",
    "@Author      :   Yan Ding \n",
    "@Time        :   2023/09/03 21:54:15\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "pip install openai==0.27.0\n",
    "pip install inflect\n",
    "pip install modelscope -U\n",
    "pip install transformers accelerate tiktoken -U\n",
    "pip install einops transformers_stream_generator -U\n",
    "pip install \"pillow==9.*\" -U\n",
    "pip install torchvision\n",
    "pip install matplotlib -U\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import inflect\n",
    "import sys\n",
    "import openai\n",
    "from modelscope import snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "995c39f1-23fd-44e2-9618-fbc2ea5b9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.0: Configuration and assumptions\n",
    "# ----------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    \"api_key\": \"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\",\n",
    "    \"gpt_model\": \"text-davinci-003\",\n",
    "    \"model_id\": \"qwen/Qwen-VL-Chat\",\n",
    "    \"revision\": \"v1.0.3\"\n",
    "}\n",
    "openai.api_key = CONFIG[\"api_key\"]\n",
    "\n",
    "\n",
    "# We have an assumption that we know object categories, and exact object name\n",
    "# 数据初始化\n",
    "fruit_names = [\"banana\", \"apple\", \"pear\"]\n",
    "food_names = [\"potato_chip\", \"pastry\", \"pie\"]\n",
    "beverage_names = ['bottle']\n",
    "utensil_names = ['knife', 'fork', 'spoon']\n",
    "container_names = ['bowl', 'mug', 'cup', 'plate']\n",
    "table_name = [\"counter\", \"table\"]\n",
    "cabinet_name = [\"cabinet\"]\n",
    "appliance_names = ['fridge', 'microwave']\n",
    "object_names = fruit_names + food_names + beverage_names + utensil_names + container_names + table_name + cabinet_name + appliance_names\n",
    "\n",
    "# 将对象按类型分组\n",
    "object_groups = {\n",
    "    'fruit_names': fruit_names,\n",
    "    'food_names': food_names,\n",
    "    'beverage_names': beverage_names,\n",
    "    'utensil_names': utensil_names,\n",
    "    'container_names': container_names,\n",
    "    'table_name': table_name,\n",
    "    'cabinet_name': cabinet_name,\n",
    "    'appliance_names': appliance_names\n",
    "}\n",
    "\n",
    "# 可能的位置列表\n",
    "locations = container_names + table_name + cabinet_name + appliance_names\n",
    "\n",
    "predicates = {\n",
    "    'fruit_names': ['on', 'in'],\n",
    "    'food_names': ['on', 'in'],\n",
    "    'beverage_names': ['on', 'in'],\n",
    "    'utensil_names': ['on', 'in'],\n",
    "    'container_names': ['empty'],\n",
    "    'table_name': [],\n",
    "    'cabinet_name': ['open'],\n",
    "    'appliance_names': ['open']\n",
    "}\n",
    "\n",
    "all_predicate_based_prompts = []\n",
    "# 遍历所有对象和位置生成提示\n",
    "for obj_group, obj_list in object_groups.items():\n",
    "    for obj in obj_list:\n",
    "        for loc in locations:\n",
    "            # 过滤掉 'in counter' 和 'in table'\n",
    "            if 'on' in predicates[obj_group] and loc in table_name:\n",
    "                all_predicate_based_prompts.append([f\"is {obj} on {loc}?\", f\"{obj}\", f\"{loc}\"])\n",
    "            if 'in' in predicates[obj_group] and loc not in table_name:\n",
    "                all_predicate_based_prompts.append([f\"is {obj} in {loc}?\", f\"{obj}\", f\"{loc}\"])\n",
    "        if 'empty' in predicates[obj_group]:\n",
    "            all_predicate_based_prompts.append([f\"is {obj} empty?\", f\"{obj}\"])\n",
    "        if 'open' in predicates[obj_group]:\n",
    "            all_predicate_based_prompts.append([f\"is {obj} open?\", f\"{obj}\"])\n",
    "\n",
    "# # 输出结果\n",
    "# for prompt in all_predicate_based_prompts:\n",
    "#     print(prompt)\n",
    "\n",
    "# 创建同义词或映射字典\n",
    "synonyms_dict = {\n",
    "    \"refrigerator freezer\": \"fridge\",\n",
    "    \"refrigerator\": \"fridge\",\n",
    "    \"freezer\": \"fridge\",\n",
    "    \"microwave oven\": \"microwave\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa50ce94-c7a0-4617-80a8-ddce295f98e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 01:27:57,862 - modelscope - INFO - Use user-specified model revision: v1.0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56bcd1ea3884ecbbeee415b08d260b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights is in /root/.cache/modelscope/hub/qwen/Qwen-VL-Chat\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.1: Load VLM models\n",
    "# ----------------------------------------------------------------\n",
    "model_dir = snapshot_download(CONFIG[\"model_id\"], revision=CONFIG[\"revision\"])\n",
    "torch.manual_seed(1234)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "print(f\"model weights is in {model_dir}\")\n",
    "# 'Users can modify configuration in generation_config.json'\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5feefba3-01a0-48b5-b2bb-e2161725e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.2: Utils functions\n",
    "# ----------------------------------------------------------------\n",
    "def extract_bounding_box(): #TODO\n",
    "    for object_index in range(len(objects_name)):\n",
    "        object = objects_in_description[object_index]\n",
    "        response, history = model.chat(tokenizer, f'please output the bounding box of {object}', history=history)\n",
    "        print(response)\n",
    "        image = tokenizer.draw_bbox_on_latest_picture(response, history)\n",
    "        image.save(f'/home/image_output/view_{camera_index}_object_{object_index}.jpg')\n",
    "\n",
    "def find_image_path(image_name):\n",
    "    \"\"\"\n",
    "    Helper function to find the image path.\n",
    "    \"\"\"\n",
    "    for ext in ['.jpg', '.png']:\n",
    "        image_path = f'/home/input/{image_name}{ext}'\n",
    "        if os.path.exists(image_path):\n",
    "            return image_path\n",
    "    raise FileNotFoundError(f\"No image found for {image_name} in supported formats (.jpg, .png).\")\n",
    "\n",
    "\n",
    "def vlm(image_name, prompt):\n",
    "    \"\"\"\n",
    "    This function queries a Vision-Language Model (VLM) for a given image name and prompt.\n",
    "    This function deos not need need history.\n",
    "    \n",
    "    Args:\n",
    "        image_name: Name of the image file without extension. The function will attempt to find both .jpg and .png extensions.\n",
    "        prompt: The text prompt/question that you want to ask the VLM about the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_path = find_image_path(image_name)\n",
    "    query = tokenizer.from_list_format([{'image': image_path}, {'text': prompt}])\n",
    "    response, history = model.chat(tokenizer, query=query, history=None)\n",
    "    \n",
    "    return response, history\n",
    "\n",
    "def vlm_history(prompt, image_history):\n",
    "    \"\"\"\n",
    "    Queries a Vision-Language Model (VLM) using a given image and prompt, while considering previous chat history.\n",
    "    \n",
    "    Args:\n",
    "        image_name (str): The base name of the image without its file extension.\n",
    "                        This function attempts to locate the image using both .jpg and .png extensions.\n",
    "        image_history (list): Previous interactions with the VLM. Expected to be tokenized.\n",
    "        prompt (str): The text prompt/question for the VLM regarding the image.\n",
    "    \"\"\"\n",
    "    response, history = model.chat(tokenizer, query=prompt, history=image_history)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def convert_prompt_to_format(initial_state):\n",
    "    \"\"\"\n",
    "    Convert the initial state prompts into a structured format based on their content.\n",
    "    For example, a prompt like 'is spoon on counter?' --> ['on', 'spoon', 'counter']\n",
    "    \n",
    "    Args:\n",
    "        initial_state (dict): A dictionary where the keys are camera identifiers (or similar) and the values \n",
    "                            are lists of prompts related to that camera.\n",
    "    \"\"\"\n",
    "    def parse_prompt(prompt):\n",
    "        words = prompt.split()\n",
    "        \n",
    "        # Check for 'on' or 'in' predicate\n",
    "        if ' on ' in prompt:\n",
    "            return ['on', words[1], words[3].replace('?', '')]\n",
    "        elif ' in ' in prompt:\n",
    "            return ['in', words[1], words[3].replace('?', '')]\n",
    "        elif ' open' in prompt:\n",
    "            return ['open', words[1].replace('?', '')]\n",
    "        elif ' empty' in prompt:\n",
    "            return ['empty', words[1].replace('?', '')]\n",
    "        else:\n",
    "            # If the prompt doesn't match any of the known formats, return the original prompt as a single string\n",
    "            return [prompt]\n",
    "\n",
    "    formatted_state = {}\n",
    "    for camera, prompts in initial_state.items():\n",
    "        formatted_prompts = [parse_prompt(prompt) for prompt in prompts]\n",
    "        formatted_state[camera] = formatted_prompts\n",
    "\n",
    "    return formatted_state\n",
    "\n",
    "def llm(prompt):\n",
    "  sampling_params = {\"n\": 1,  # sampling number\n",
    "                      \"max_tokens\": 32,\n",
    "                      \"temperature\": 0,\n",
    "                      \"top_p\": 1,\n",
    "                      \"logprobs\": 1,\n",
    "                      \"presence_penalty\": 0,\n",
    "                      \"frequency_penalty\": 0,\n",
    "                      \"stop\": ['\\\\n', '.']}\n",
    "  raw_response = openai.Completion.create(engine=CONFIG[\"gpt_model\"], prompt=prompt, **sampling_params)\n",
    "  responses = [raw_response['choices'][i]['text'] for i in range(sampling_params['n'])]\n",
    "  mean_probs = [math.exp(np.mean(raw_response['choices'][i]['logprobs']['token_logprobs'])) for i in range(sampling_params['n'])]\n",
    "  responses = [sample.strip().lower() for sample in responses]\n",
    "  return responses, mean_probs\n",
    "\n",
    "def query_llm_for_objects(response):\n",
    "    task = \"The task is to extract object names in the sentence.\"\n",
    "    example1 = \"Input: There is an apple and a bowl. Output: apple, bowl\"\n",
    "    example2 = \"Input: There is an oven on the counter. Output: oven, counter\"\n",
    "    question = f\"Input: {response}\"\n",
    "    prompt = f'{task} \\n {example1} \\n {example2} \\n {question}.'\n",
    "    response, _ = llm(prompt)\n",
    "\n",
    "    items = response[0].split(':')[-1]\n",
    "    items_list = [item.strip() for item in items.split(',') if item.strip()]\n",
    "    normalized_items = [synonyms_dict.get(item, item) for item in items_list]\n",
    "\n",
    "    p = inflect.engine()\n",
    "    return [p.singular_noun(item) or item for item in normalized_items if any(item in obj_name for obj_name in object_names)]\n",
    "\n",
    "def select_prompts(objects):\n",
    "    return [\n",
    "        sentence[0] \n",
    "        for sentence in all_predicate_based_prompts \n",
    "        if (len(sentence) == 3 and sentence[1] in objects and sentence[2] in objects) or\n",
    "           (len(sentence) != 3 and sentence[1] in objects)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c80bd5f8-634d-43fa-803d-e34f8a4bd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM has output response about image captioning!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.3: Do image captioning\n",
    "# ----------------------------------------------------------------\n",
    "vlm_responses = {}  # store responses from vlm\n",
    "histories = {}  # store histories from vlm\n",
    "num_view = 6\n",
    "for index in range(num_view):\n",
    "    index += 1\n",
    "    image_name = f'view_{index}'\n",
    "    prompt = 'please descripe the input image.'\n",
    "    response, history = vlm(image_name, prompt)\n",
    "    \n",
    "    # store results\n",
    "    vlm_responses[image_name] = response\n",
    "    histories[image_name] = history\n",
    "\n",
    "print('VLM has output response about image captioning!')\n",
    "# print(f'vlm_responses:{vlm_responses}')\n",
    "# print(f'histories:{histories}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca7f6822-b891-41f9-b87b-4f3e38467e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in image view_1: ['counter', 'apple']\n",
      "Objects in image view_2: ['microwave']\n",
      "Objects in image view_3: ['microwave']\n",
      "Objects in image view_4: ['cabinet']\n",
      "Objects in image view_5: ['fridge', 'microwave']\n",
      "Objects in image view_6: ['fridge']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.4: Extract objects from VLM's response\n",
    "# ----------------------------------------------------------------\n",
    "extracted_objects = {}\n",
    "for image_name, response in vlm_responses.items():\n",
    "  extracted_objects[image_name] = query_llm_for_objects(response)\n",
    "  print('Objects in image {}: {}'.format(image_name, extracted_objects[image_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08e5a7eb-fc86-4264-a94c-f9eca34e0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For view_1, predicate_based_prompts: ['in the image, is apple on counter?']\n",
      "For view_2, predicate_based_prompts: ['in the image, is microwave open?']\n",
      "For view_3, predicate_based_prompts: ['in the image, is microwave open?']\n",
      "For view_4, predicate_based_prompts: ['in the image, is cabinet open?']\n",
      "For view_5, predicate_based_prompts: ['in the image, is fridge open?', 'in the image, is microwave open?']\n",
      "For view_6, predicate_based_prompts: ['in the image, is fridge open?']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.5: Select predicate-based prompts\n",
    "# ----------------------------------------------------------------\n",
    "selected_predicate_based_prompts = {}\n",
    "\n",
    "for image_name, objects in extracted_objects.items():\n",
    "    matched_prompts = []\n",
    "    for sentence in all_predicate_based_prompts:\n",
    "        object1 = sentence[1]\n",
    "        object2 = sentence[2] if len(sentence) == 3 else None\n",
    "\n",
    "        # 若object1在extracted_objects中，并且object2在extracted_objects中或者为None，那么将此句子加入matched_prompts\n",
    "        if object1 in objects and (not object2 or object2 in objects):\n",
    "            matched_prompts.append(sentence[0])\n",
    "\n",
    "    selected_predicate_based_prompts[image_name] = matched_prompts\n",
    "\n",
    "for image_name, predicate_based_prompts in selected_predicate_based_prompts.items():\n",
    "    print(f'For {image_name}, predicate_based_prompts: {predicate_based_prompts}')\n",
    "\n",
    "# print(f'selected_predicate_based_prompts={selected_predicate_based_prompts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1100c021-f42c-4cc2-9b6c-7775e64c5b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:Yes, there is an apple on the counter in the image.\n",
      "response:No, the microwave is closed in the image.\n",
      "response:Yes, the microwave is open in the image.\n",
      "response:Yes, the cabinet is open in the image.\n",
      "response:Yes, the refrigerator is open in the image.\n",
      "response:No, the microwave is closed in the image.\n",
      "response:Yes, the fridge is open in the image.\n",
      "initial_state={'view_1': [['on', 'the', 'is']], 'view_3': [['open', 'the']], 'view_4': [['open', 'the']], 'view_5': [['open', 'the']], 'view_6': [['open', 'the']]}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Step 4.6: Convert prompts\n",
    "# ----------------------------------------------------------------\n",
    "initial_state = {}\n",
    "for index_view in range(num_view):\n",
    "    index_view = f'view_{index_view + 1}'\n",
    "    for index_prompt in range(len(selected_predicate_based_prompts[index_view])):\n",
    "        prompt = selected_predicate_based_prompts[index_view][index_prompt]\n",
    "        image_history = histories[index_view]\n",
    "        response = vlm_history(prompt, image_history)\n",
    "        print(f'response:{response}')\n",
    "        \n",
    "        if 'yes' in response.lower():\n",
    "            if image_name not in initial_state:\n",
    "                initial_state[index_view] = []\n",
    "            initial_state[index_view].append(prompt)\n",
    "\n",
    "formatted_state = convert_prompt_to_format(initial_state)\n",
    "print(f'initial_state={formatted_state}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
